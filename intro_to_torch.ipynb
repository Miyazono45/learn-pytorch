{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Miyazono45/learn-pytorch/blob/main/intro_to_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7FeunEw-okx"
      },
      "source": [
        "# Simple NN (We'll use Iris Dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mwipKy4_H1Kj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# ct = torch.randn(2,3,4)\n",
        "# print(ct)\n",
        "\n",
        "# cr = torch.randn(2,2,15)\n",
        "# print(cr)\n",
        "# cr = cr.reshape(-1,3)\n",
        "# print(cr)\n",
        "# crx = cr[:,-1:]\n",
        "# print(crx)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWfRKvR08Fji"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "  def __init__(self, input_feature = 3, fh1 = 6, fh2 = 6, output_feature = 3):\n",
        "    super().__init__()\n",
        "    self.fc1 = nn.Linear(input_feature, fh1)\n",
        "    self.fc2 = nn.Linear(fh1, fh2)\n",
        "    self.out = nn.Linear(fh2, output_feature)\n",
        "\n",
        "  # to keep NN forwarding\n",
        "  def forward(self, x):\n",
        "    y = F.relu(self.fc1(x))\n",
        "    y = F.relu(self.fc2(y)) # Fixed: Changed x to y\n",
        "    y = self.out(y) # Fixed: Changed x to y\n",
        "\n",
        "    return y\n",
        "\n",
        "# Randomize\n",
        "torch.manual_seed(22)\n",
        "model = Model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6S-U3kT1MXQB"
      },
      "outputs": [],
      "source": [
        "# Dataset for train\n",
        "url_dataset = \"/content/dataset/flower_dataset.csv\"\n",
        "df = pd.read_csv(url_dataset)\n",
        "\n",
        "# Remap the species, size, fragrance value into float\n",
        "df['fragrance'] = df['fragrance'].apply(lambda species: float(1.0) if species == \"strong\" else float(0.5) if species == \"mild\" else float(0.0))\n",
        "df['size'] = df['size'].apply(lambda size: float(1.0) if size == \"large\" else float(0.5) if size == \"medium\" else float(0.0))\n",
        "df['species'] = df['species'].apply(lambda species: float(1.0) if species == \"rose\" else float(0.5) if species == \"shoeblack plant\" else float(0.0))\n",
        "\n",
        "train_data = df.drop('species', axis=1)\n",
        "test_data = df['species']\n",
        "\n",
        "train_data_value = train_data.values\n",
        "test_data_value = test_data.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nuQVbPBwOcRJ"
      },
      "outputs": [],
      "source": [
        "# Splitting train and test data\n",
        "x_train, x_test, y_train, y_test = train_test_split(train_data_value, test_data_value, test_size=0.25, random_state=22)\n",
        "\n",
        "# Convert train and test data into tensor\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "x_test = torch.FloatTensor(x_test)\n",
        "\n",
        "y_train = torch.LongTensor(y_train)\n",
        "y_test = torch.LongTensor(y_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "soGKpPN6u3Ql"
      },
      "outputs": [],
      "source": [
        "# Set criterion (measure the error), how far off the predictions are from the data\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# Using Adam Optimizer, lr = learning rate (if error doesn't go down after iteration, then lower the LR)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-WRBGBZxfH6"
      },
      "outputs": [],
      "source": [
        "#  Train Model\n",
        "epochs = 500\n",
        "losses = []\n",
        "\n",
        "for i in range(epochs):\n",
        "  # Forward and get prediction\n",
        "  test_pred = model.forward(x_train)\n",
        "  # Measure loss\n",
        "  loss = criterion(test_pred, y_train)\n",
        "  # Track loss\n",
        "  losses.append(loss.detach().numpy())\n",
        "\n",
        "  # Print epoch\n",
        "  if i % 10 == 0:\n",
        "    print(f'Epoch {i} with loss {loss}')\n",
        "\n",
        "  # Back propragation\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNJi5mtn0I9e"
      },
      "outputs": [],
      "source": [
        "# Graph the losses\n",
        "plt.plot(range(epochs), losses)\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.xlabel(\"Epoch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slH69P60UP4a"
      },
      "outputs": [],
      "source": [
        "# Evaluate Model\n",
        "with torch.no_grad():\n",
        "  y_eval = model.forward(x_test)\n",
        "  loss = criterion(y_eval, y_test) # find differet from evaluate and test\n",
        "\n",
        "# Show correct\n",
        "correct = 0\n",
        "with torch.no_grad():\n",
        "  for i, data in enumerate(x_test):\n",
        "    y_val = model.forward(data)\n",
        "    print(f'{i+1}. {str(y_val)} {y_test[i]}')\n",
        "\n",
        "    if y_val.argmax().item() == y_test[i]:\n",
        "      correct += 1\n",
        "\n",
        "\n",
        "print(f'Accuracy: {correct/len(y_test)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMRID9todh9D"
      },
      "source": [
        "# CNN (Conv Neural Network) with MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AnsLSxALdhKH"
      },
      "outputs": [],
      "source": [
        "import torch, os\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJwd9ANUhQWX"
      },
      "outputs": [],
      "source": [
        "!rm -rf '/root/.cache/kagglehub/datasets/imsparsh/flowers-dataset'\n",
        "\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"imsparsh/flowers-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sU7Hsf451Ttg"
      },
      "outputs": [],
      "source": [
        "# Convert kaggle dataset into Tensor 4 Dimension\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Resize((224, 224))\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8QcZ0Z6T76_T"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class ImageTrainDataset(Dataset): # In this case, i don't have dataset with labeled csv, so we need to create itself!\n",
        "  def __init__(self, root_dir=None, transform=None):\n",
        "    self.root_dir = root_dir    # in this case in \"train\" folder\n",
        "    self.transform = transform  # transform to augment\n",
        "\n",
        "    # Create list class and index itself (ie daisy: 0, sunflower:1)\n",
        "    self.classFolder = sorted(folder.name for folder in os.scandir(self.root_dir) if folder.is_dir()) # get all flower folder name in \"train\"\n",
        "    self.classIndex = {class_name: index for index, class_name in enumerate(self.classFolder)}        # create index for each class {daisy: 1}\n",
        "\n",
        "    # Collect all image path and combine with labels\n",
        "    self.img_path = []\n",
        "    self.label_img = []\n",
        "\n",
        "    for class_name in self.classFolder:\n",
        "      class_path = os.path.join(self.root_dir, class_name) # \"train/daisy\"\n",
        "      for img_name in os.listdir(class_path): # search all file inside \"train/daisy\"\n",
        "        self.img_path.append(os.path.join(class_path, img_name))  # \"train/daisy/img_a.jpg\"\n",
        "        self.label_img.append(self.classIndex[class_name])        # get 0,1,2,3 based on classIndex (classIndex['daisy'])\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.img_path)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    image_path = self.img_path[index]\n",
        "    y_label = self.label_img[index]\n",
        "    images = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "    if self.transform:\n",
        "      images = self.transform(images)\n",
        "\n",
        "    return images, y_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8le_FVJPBf7C"
      },
      "outputs": [],
      "source": [
        "# Custom Dataset Class\n",
        "class ImageTestDataset(Dataset): # In this case, i don't have dataset with labeled csv, so we need to create itself!\n",
        "  def __init__(self, csv_file, train_dir, test_dir, transform=None):\n",
        "    self.csv_file = csv_file\n",
        "    self.train_dir = train_dir\n",
        "    self.test_dir = test_dir\n",
        "    self.transform = transform\n",
        "\n",
        "    self.classTrainFolder = sorted(folder.name for folder in os.scandir(self.train_dir) if folder.is_dir()) # get all flower folder name in \"train\"\n",
        "    self.classTrainIndex = {class_name: index for index, class_name in enumerate(self.classTrainFolder)}        # create index for each class {daisy: 1}\n",
        "\n",
        "    # Collect all image path and combine with labels\n",
        "    self.test_img_path = []\n",
        "    self.label_img = []\n",
        "\n",
        "    # Loop to collect img_path in test_dir and get label from csv_file\n",
        "    for img_file in os.listdir(self.test_dir):\n",
        "      self.test_img_path.append(os.path.join(self.test_dir, img_file))  #  Append \"test/Image_1.jpg\"\n",
        "      label = self.csv_file[self.csv_file[0] == img_file][1].iloc[0]    # see csv_file > is current row match file current file_name > select second column (label \"sunflower\") > get first value (or current value)\n",
        "      self.label_img.append(self.classTrainIndex[label])                # Append sunflower[label] (3)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.test_img_path)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    test_image_path = self.test_img_path[index]\n",
        "    test_images = Image.open(test_image_path).convert(\"RGB\")\n",
        "    y_label = self.label_img[index]\n",
        "\n",
        "    if self.transform:\n",
        "      test_images = self.transform(test_images)\n",
        "\n",
        "    return test_images, y_label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E4kq_MdIOysS"
      },
      "outputs": [],
      "source": [
        "# DataLoader\n",
        "# Since we'll split the train data and test data, we don't need train_test_split from sklearn\n",
        "dataset_train = ImageTrainDataset(root_dir=\"/kaggle/input/flowers-dataset/train\", transform=transform)\n",
        "loader_train = DataLoader(dataset_train, batch_size=16, shuffle=True)\n",
        "\n",
        "df_test = pd.concat(map(pd.read_csv, ['/kaggle/input/flowers-dataset/Testing_set_flower.csv', '/kaggle/input/flowers-dataset/sample_submission.csv']), ignore_index=True, axis=1)\n",
        "dataset_test = ImageTestDataset(csv_file=df_test, train_dir=\"/kaggle/input/flowers-dataset/train\", test_dir=\"/kaggle/input/flowers-dataset/test\", transform=transform)\n",
        "loader_test = DataLoader(dataset_test, batch_size=8, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qpf89PNXWAIp"
      },
      "outputs": [],
      "source": [
        "# JUST TESTING SOME CNN HEHE (IGNORE THIS)\n",
        "z = 0\n",
        "conv1 = nn.Conv2d(3, 6, 3)\n",
        "conv2 = nn.Conv2d(6, 12, 3)\n",
        "for a, b in loader_train:\n",
        "  # Image.open(index).convert('RGB')\n",
        "  print(f'a = {a.shape}')\n",
        "  print(f'b = {b}')\n",
        "\n",
        "  a = F.relu(conv1(a))\n",
        "  a = F.max_pool2d(a, 2, 2)\n",
        "  a = F.relu(conv2(a))\n",
        "  a = F.max_pool2d(a, 2, 2)\n",
        "  print(a.shape)\n",
        "\n",
        "  z = z + 1\n",
        "\n",
        "  if z >= 2:\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SHhR61YwbmX"
      },
      "outputs": [],
      "source": [
        "# Model Class\n",
        "class ConvolutionalNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    # Conv\n",
        "    self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1) # Channel, Input, padding, Stride\n",
        "    # SINCE THIS CODE IN CONV SECTION, WE WILL USE nn.MaxPool2d, because F.max_pool2d is for forwarding\n",
        "    self.pool1 = nn.MaxPool2d(2, 2) # we will compress / pooling this by 2\n",
        "    self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1) # Because conv1 out_feature is 32, so we will use this for in_feature and mult by 2 to out_feature\n",
        "    self.pool2 = nn.MaxPool2d(2, 2)\n",
        "    self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1) # Same as before, it multiplied by 2\n",
        "    self.pool3 = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    # Fully Connected Node (I DONT UNDERSTAND THIS, I'M BURNING OUT 😭)\n",
        "    self.fc1 = nn.Linear(128 * 26 * 26, 512) # why is this 26 lmaoooo,\n",
        "    # or maybe i know, it's because the first input (my pixel image 224) is 224, so when it got conv1, it down to (224-kernelsize)/stride+1 = 222 > pool1 = 111 > conv2 = 109 > pool2 = 54 > conv3 = 52 > pool3 = 26\n",
        "\n",
        "    self.fc2 = nn.Linear(512, 5) # why is this 512???\n",
        "    # 512 is very arbitary value, we can choose it start from 512, 1024, 2048, or something\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = F.relu(self.conv1(x))\n",
        "    x = self.pool1(x)\n",
        "    x = F.relu(self.conv2(x))\n",
        "    x = self.pool2(x)\n",
        "    x = F.relu(self.conv3(x))\n",
        "    x = self.pool3(x)\n",
        "\n",
        "    # Flatten (what is this 😭)\n",
        "    # Flatten it's like we laying all down value into 1 row\n",
        "    x = x.view(x.size(0), -1)  # flatten (reshaped into one line)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = self.fc2(x)\n",
        "    return x\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f6262e99"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available and being used.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, using CPU.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJxHIiMc9amK"
      },
      "outputs": [],
      "source": [
        "# Create instance Model\n",
        "torch.manual_seed(22)\n",
        "cnnModel = ConvolutionalNetwork()\n",
        "# Move the model to the selected device before initializing the optimizer\n",
        "cnnModel.to(device)\n",
        "cnnModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKoR84AF-aIl"
      },
      "outputs": [],
      "source": [
        "# Loss Function Optimizer (Don't know what is this)\n",
        "cnnCriterion = nn.CrossEntropyLoss()\n",
        "cnnOptimizer = torch.optim.Adam(cnnModel.parameters(), lr=0.01)\n",
        "\n",
        "# Move optimizer state to the same device as the model\n",
        "cnnOptimizer.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKCr4RQtwSbV"
      },
      "outputs": [],
      "source": [
        "# # Train and Testing Model\n",
        "# import time\n",
        "# start_time = time.time()\n",
        "\n",
        "# # Declare variable to track epoch, loss, valid\n",
        "# epoch = 100\n",
        "# train_loss = []\n",
        "# test_loss = []\n",
        "# train_correct = []\n",
        "# test_correct = []\n",
        "\n",
        "# # Loop through epochs\n",
        "# for i in range(epoch):\n",
        "#   train_correct_count = 0\n",
        "#   test_correct_count = 0\n",
        "\n",
        "#   # Train\n",
        "#   for batch, (x_train_b, y_train_b) in enumerate(loader_train):\n",
        "#     batch += 1\n",
        "#     # Move data to the device\n",
        "#     x_train_b, y_train_b = x_train_b.to(device), y_train_b.to(device)\n",
        "\n",
        "#     y_pred = cnnModel(x_train_b)\n",
        "#     loss = cnnCriterion(y_pred, y_train_b)\n",
        "\n",
        "#     predicted = torch.max(y_pred.data, 1)[1]\n",
        "#     batch_correct = (predicted == y_train_b).sum()\n",
        "#     # train_loss.append(loss.item())\n",
        "#     train_correct_count += batch_correct\n",
        "\n",
        "#     # Update Parameter\n",
        "#     cnnOptimizer.zero_grad()\n",
        "#     loss.backward()\n",
        "#     cnnOptimizer.step()\n",
        "\n",
        "#     # Print result\n",
        "#     # if batch%600 == 0:\n",
        "#     print(f'Epoch: {i} Batch: {batch} Loss: {loss.item()}')\n",
        "\n",
        "#   train_loss.append(loss.item())\n",
        "#   train_correct.append(train_correct_count)\n",
        "\n",
        "#   # Test\n",
        "#   with torch.no_grad():\n",
        "#     for batch, (x_test_b, y_test_b) in enumerate(loader_test):\n",
        "#       # Move data to the device\n",
        "#       x_test_b, y_test_b = x_test_b.to(device), y_test_b.to(device)\n",
        "\n",
        "#       y_val = cnnModel(x_test_b)\n",
        "#       predicted = torch.max(y_val.data, 1)[1]\n",
        "#       test_correct_count += (predicted == y_test_b).sum()\n",
        "\n",
        "#     loss = cnnCriterion(y_val, y_test_b)\n",
        "#     test_loss.append(loss.item())\n",
        "#     test_correct.append(test_correct_count)\n",
        "\n",
        "\n",
        "# current_time = time.time()\n",
        "# total = current_time - start_time\n",
        "# print(f'Total Time: {total/60} minutes')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "828e7487"
      },
      "source": [
        "First, let's check if a GPU is available in your environment."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84022cca"
      },
      "source": [
        "Now, let's move your model and data to the GPU (or CPU if GPU is not available)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c90ab739"
      },
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available and define the device\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"GPU is available and being used.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU is not available, using CPU.\")\n",
        "\n",
        "# Move the model to the selected device\n",
        "cnnModel.to(device)\n",
        "\n",
        "# Modify your training and testing loops to move data to the device as well\n",
        "# For example, in your training loop:\n",
        "# for batch, (x_train_b, y_train_b) in enumerate(loader_train):\n",
        "#     x_train_b, y_train_b = x_train_b.to(device), y_train_b.to(device)\n",
        "#     ...\n",
        "\n",
        "# And in your testing loop:\n",
        "# with torch.no_grad():\n",
        "#     for batch, (x_test_b, y_test_b) in enumerate(loader_test):\n",
        "#         x_test_b, y_test_b = x_test_b.to(device), y_test_b.to(device)\n",
        "#         ...\n",
        "\n",
        "print(\"Model and data moved to:\", device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e89e2a5"
      },
      "source": [
        "# Train and Testing Model\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# Declare variable to track epoch, loss, valid\n",
        "epoch = 100\n",
        "train_loss = []\n",
        "test_loss = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "\n",
        "# Loop through epochs\n",
        "for i in range(epoch):\n",
        "  train_correct_count = 0\n",
        "  test_correct_count = 0\n",
        "\n",
        "  # Train\n",
        "  for batch, (x_train_b, y_train_b) in enumerate(loader_train):\n",
        "    batch += 1\n",
        "    # Move data to the device\n",
        "    x_train_b, y_train_b = x_train_b.to(device), y_train_b.to(device)\n",
        "\n",
        "    y_pred = cnnModel(x_train_b)\n",
        "    loss = cnnCriterion(y_pred, y_train_b)\n",
        "\n",
        "    predicted = torch.max(y_pred.data, 1)[1]\n",
        "    batch_correct = (predicted == y_train_b).sum()\n",
        "    # train_loss.append(loss.item())\n",
        "    train_correct_count += batch_correct\n",
        "\n",
        "    # Update Parameter\n",
        "    cnnOptimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    cnnOptimizer.step()\n",
        "\n",
        "    # Print result\n",
        "    # if batch%600 == 0:\n",
        "    print(f'Epoch: {i} Batch: {batch} Loss: {loss.item()}')\n",
        "\n",
        "  train_loss.append(loss.item())\n",
        "  train_correct.append(train_correct_count)\n",
        "\n",
        "  # Test\n",
        "  with torch.no_grad():\n",
        "    for batch, (x_test_b, y_test_b) in enumerate(loader_test):\n",
        "      # Move data to the device\n",
        "      x_test_b, y_test_b = x_test_b.to(device), y_test_b.to(device)\n",
        "\n",
        "      y_val = cnnModel(x_test_b)\n",
        "      predicted = torch.max(y_val.data, 1)[1]\n",
        "      test_correct_count += (predicted == y_test_b).sum()\n",
        "\n",
        "    loss = cnnCriterion(y_val, y_test_b)\n",
        "    test_loss.append(loss.item())\n",
        "    test_correct.append(test_correct_count)\n",
        "\n",
        "\n",
        "current_time = time.time()\n",
        "total = current_time - start_time\n",
        "print(f'Total Time: {total/60} minutes')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4fe1f02"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "z7FeunEw-okx"
      ],
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPGy1XrC3/bTttMyQF2ga4A",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}